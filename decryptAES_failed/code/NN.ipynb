{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def loaddatasets(num_train):\n",
    "    all_data = pickle.load(open('all_data'))\n",
    "    all_label = pickle.load(open('all_label'))\n",
    "    train_data = all_data[:num_train].astype(np.float32)\n",
    "    train_label = all_label[:num_train, -5:].astype(np.int32)\n",
    "    test_data = all_data[num_train:].astype(np.float32)\n",
    "    test_label =  all_label[num_train:, -5].astype(np.int32)\n",
    "    return train_data, train_label, test_data, test_label\n",
    "\n",
    "\n",
    "def acc(predictions,label):\n",
    "    return  (100.0 * np.sum((np.array(predictions).reshape(1,-1)+0.5).astype(np.int32)==label.reshape(1,-1)) / float(predictions.shape[0]*5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = loaddatasets(900000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49944888888888889"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_label)/float(train_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900000, 128) (100000, 128)\n",
      "DATA LOADED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_data, train_label, test_data, test_label = pickle.load(open('smalldata'))\n",
    "print (train_data.shape, test_data.shape)\n",
    "\n",
    "#print(train_data)\n",
    "#print(test_label)\n",
    "print ('DATA LOADED')\n",
    "batch_size = 128\n",
    "beta = 0.000\n",
    "\n",
    "h1_nodes = 128\n",
    "h2_nodes = 512\n",
    "h3_nodes = 512\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 128))\n",
    "    tf_train_label = tf.placeholder(tf.float32, shape=(batch_size, 5))\n",
    "    tf_test_dataset = tf.constant(train_data)\n",
    "\n",
    "    # H1\n",
    "    h1_w = tf.Variable(tf.truncated_normal([128, h1_nodes]))\n",
    "    h1_b = tf.Variable(tf.truncated_normal([h1_nodes]))\n",
    "    h1 = tf.nn.sigmoid(tf.matmul(tf_train_dataset, h1_w) + h1_b)\n",
    "\n",
    "    # H2\n",
    "#     h2_w = tf.Variable(tf.truncated_normal([h1_nodes, h2_nodes]))\n",
    "#     h2_b = tf.Variable(tf.truncated_normal([h2_nodes]))\n",
    "#     h2 = tf.nn.relu(tf.matmul(h1, h2_w) + h2_b)\n",
    "\n",
    "    #H3\n",
    "#     h3_w = tf.Variable(tf.truncated_normal([h2_nodes, h3_nodes]))\n",
    "#     h3_b = tf.Variable(tf.truncated_normal([h3_nodes]))\n",
    "#     h3 = tf.nn.relu(tf.matmul(h2, h3_w) + h3_b)\n",
    "\n",
    "    # last\n",
    "\n",
    "    w = tf.Variable(tf.truncated_normal([h1_nodes, 5]))\n",
    "    b = tf.Variable(tf.truncated_normal([5]))\n",
    "\n",
    "    logits = tf.matmul(h1, w)+b\n",
    "\n",
    "    loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits,tf_train_label) + beta * tf.nn.l2_loss(w))\n",
    "\n",
    "    optimizer= tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "    test_relu1= tf.nn.sigmoid(tf.matmul(tf_test_dataset,h1_w)+h1_b)\n",
    "#     test_relu2= tf.nn.relu(tf.matmul(test_relu1,h2_w)+h2_b)\n",
    "#     test_relu3= tf.nn.relu(tf.matmul(test_relu2,h3_w)+h3_b)\n",
    "    test_prediction= tf.nn.sigmoid(tf.matmul(test_relu1,w)+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF INIT\n",
      "batch ACC 0 :51.562500\n",
      "Test ACC 50.0\n",
      "batch ACC 500 :50.781250\n",
      "Test ACC 50.0\n",
      "batch ACC 1000 :49.843750\n",
      "Test ACC 50.0\n",
      "batch ACC 1500 :47.812500\n",
      "Test ACC 50.0\n",
      "batch ACC 2000 :47.968750\n",
      "Test ACC 50.0\n",
      "batch ACC 2500 :48.593750\n",
      "Test ACC 50.0\n",
      "batch ACC 3000 :50.156250\n",
      "Test ACC 50.0\n",
      "batch ACC 3500 :50.312500\n",
      "Test ACC 50.0\n",
      "batch ACC 4000 :53.281250\n",
      "Test ACC 50.0\n",
      "batch ACC 4500 :49.843750\n",
      "Test ACC 50.0\n",
      "batch ACC 5000 :49.687500\n",
      "Test ACC 50.0\n",
      "batch ACC 5500 :48.125000\n",
      "Test ACC 50.0\n",
      "batch ACC 6000 :49.218750\n",
      "Test ACC 50.0\n",
      "batch ACC 6500 :53.593750\n",
      "Test ACC 50.0\n",
      "batch ACC 7000 :52.187500\n",
      "Test ACC 50.0\n",
      "batch ACC 7500 :50.625000\n",
      "Test ACC 50.0\n",
      "batch ACC 8000 :51.093750\n",
      "Test ACC 50.0\n",
      "batch ACC 8500 :52.968750\n",
      "Test ACC 50.1\n",
      "batch ACC 9000 :48.437500\n",
      "Test ACC 50.0\n",
      "batch ACC 9500 :47.343750\n",
      "Test ACC 50.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numstep = 10000\n",
    "with tf.Session(graph=graph) as ses:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print ('TF INIT')\n",
    "    for step in range(numstep):\n",
    "        offset = (step*batch_size) % (train_label.shape[0]-batch_size)\n",
    "        batch_data=train_data[offset:offset+batch_size,:]\n",
    "        batch_label=train_label[offset:offset+batch_size,:]\n",
    "\n",
    "        feed_dict= {tf_train_dataset:batch_data, tf_train_label:batch_label,}\n",
    "        _,l,pp = ses.run([optimizer,loss,prediction],feed_dict=feed_dict)\n",
    "        if step % 500==0:\n",
    "        \n",
    "            print (\"batch ACC %d :%f\"%(step,acc(pp,batch_label)))\n",
    "            tt=test_prediction.eval()\n",
    "            #print (train_label.reshape(-1,1)[:10])\n",
    "            #print ((np.array(tt[:10])+0.5).astype(np.int32))\n",
    "            print (\"Test ACC %.1f\"%acc(tt,train_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([0.2])+.5).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b9a5896f20e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC(verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "f=clf.fit(train_data[:100000,:],train_label[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51060000000000005"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.score(test_data[:10000,:],test_label[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
